{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8236a8b-221d-465b-89f5-7e73175f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f82eb8",
   "metadata": {},
   "source": [
    "# Define a Goal\n",
    "Our Goal is to create a Network Intrusion Detection System (NIDS).\n",
    "Categorize each object that is a raw packet as:\n",
    "- Attack or Not Attack (label 1 or 0)\n",
    "- Category of the Attack.  \n",
    "\n",
    "Using a **Supervised Classificator**, our model will be able to classify a packet captured in **Malicious** or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d9611-ccf8-48b4-9f04-a569f2cae874",
   "metadata": {},
   "source": [
    "# Data AcQuisition (DAQ)\n",
    "We used a non cleaned dataset found on kaggle.com: **UNSW-NB15**. The raw packet was created by the *'IXIA PerfectStorm tool'*. This dataset is a labeled datset and in particular has nine types of attacks: \n",
    "- Generic: Techniques used against all block-cypher.\n",
    "- Fuzzers: Send random data to find vulnerabilities.\n",
    "- Backdoors: Hidden access for unauthorized control.\n",
    "- DoS: Overwhelm a system to disrupt service.\n",
    "- Exploits: Use vulnerabilities for unauthorized access.\n",
    "- Reconnaissance: Gather info to find vulnerabilities.\n",
    "- Shellcode: Malicious code for system control.\n",
    "- Worms: Self-spreading malware across networks.\n",
    "- Analysis:  Gathering info for exploitation.\n",
    "\n",
    "First we assign a column name according to *NUSW-NB15_features.csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9432b12-177b-4a34-ba93-d2c3dd9914e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = [\n",
    "    'srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur',\n",
    "    'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service',\n",
    "    'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb',\n",
    "    'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit',\n",
    "    'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat',\n",
    "    'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n",
    "    'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',\n",
    "    'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat',\n",
    "    'Label'\n",
    "]\n",
    "df = pd.read_csv('UNSW-NB15_4.csv', header=None, names=column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480dbd9",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "The exploration of the data is made in order to identify the dataset content, to extract and visualize data. We will use this in order to indentify possible features in the dataset that we do not want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c474d0b-2d2e-4d0c-96c0-5e66ff0919ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffdce1",
   "metadata": {},
   "source": [
    "We make a describe() only for usefull feature:\n",
    "- Dur: Analyzes the duration of connections.\n",
    "- Sbytes, Dbytes: Amount of bytes exchanged between source and destination.\n",
    "- Sload, Dload: Data transmission speed.\n",
    "- Spkts, Dpkts: Number of packets sent and received.\n",
    "- Sjit, Djit: Jitter of the connection (variation in delay).\n",
    "- Sintpkt, Dintpkt: Interval between packets.\n",
    "- Tcprtt, Synack, Ackdat: TCP round-trip time, SYN-ACK, and acknowledgment time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39775f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_described = [\n",
    "    'dur', 'sbytes', 'dbytes', 'Sload', 'Dload', 'sloss', 'dloss',\n",
    "    'Spkts', 'Dpkts', 'Sjit', 'Djit',\n",
    "    'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat'\n",
    "]\n",
    "\n",
    "df[column_described].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d8869",
   "metadata": {},
   "source": [
    "## Validating value 0 for the Duration of the packet\n",
    "Value 0 in duration could be an error. let's compare it with other values relevant like *sbytes, dbytes, Sjit, Djit, sloss, or dloss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_zero = df[df['dur'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325898a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['sbytes', 'dbytes', 'Sjit', 'Djit', 'sloss', 'dloss']\n",
    "dur_zero[columns_of_interest].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01276f0",
   "metadata": {},
   "source": [
    "Instead it seems that everything is ok because every packet with *dur == 0* have a *sbytes* not zero and instead the *dbytes* are zero so no bytes are sent back to the source, this can due to a specific type of packet that do not need a response immmediately (UDP) or due to an error communication.\n",
    "No packet loss is encountered so it is ok that the duration is zero because no retrasmission is needed. Jitter in the source *Sjit* with *mean* that low *4.0 ms* is ok, probably the max *15.5 ms* is an error because we are considering *dur* 0.   \n",
    "\n",
    "We have seen that a low *jitter* is ok even if the duration is zero, this because the jitter is not related to a single packet but a record of packets that can have *dur* not equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d1a0e",
   "metadata": {},
   "source": [
    "## Label Visualization\n",
    "Explore if the dataset is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd850c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1_count = df['Label'].value_counts().get(1, 0)\n",
    "print(f'Malicious packets: {label_1_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86393ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_count = df['Label'].value_counts().get(0, 0)\n",
    "print(f'Normal packets: {label_0_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Normal packets', 'Malicious packets']\n",
    "sizes = [label_0_count, label_1_count]\n",
    "colors = ['#4CAF50', '#FF0000']  \n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, explode=(0.005, 0)) \n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80e1b7",
   "metadata": {},
   "source": [
    "The dataset is **unbalanced**.\n",
    "\n",
    "Now let's visualize the Attack Category one by one counting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_category_counts = df['attack_cat'].value_counts()\n",
    "attack_category_counts.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "attack_category_counts.plot(kind='barh', color='#FF0000')\n",
    "plt.title('Count of Instances per Attack Category')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('attack_cat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa4e0e",
   "metadata": {},
   "source": [
    "In the dataset the *label* 0 indicating not an attack does not have a specific *attack category* feature. We need to add it in order to visualize clearly the distribuition of the *label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e277b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Label'] == 0, 'attack_cat'] = 'Normal'\n",
    "\n",
    "category_counts = df['attack_cat'].value_counts()\n",
    "category_counts.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#4CAF50' if category == 'Normal' else '#FF0000' for category in category_counts.index]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts.plot(kind='barh', color=colors)\n",
    "plt.title('Count of Instances per Attack Category with Normal Packets')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('attack_cat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "category_counts.plot(kind='barh', logx = True, color=colors)\n",
    "plt.title('Count of Instances per Attack Category with Normal Packets (Logarithmic Scale)')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('attack_cat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0239ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['attack_cat'].isin(['DoS'])]\n",
    "filtered_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84909bda",
   "metadata": {},
   "source": [
    "Now our idea is map the *attack_cat* into 4 different category used commonly in intrusion detection:\n",
    "- DoS: DoS, Worms\n",
    "- U2R: Backdoor, Shellcode\n",
    "- R2L: Exploits, Analysis\n",
    "- Probe: Reconnaissance, Fuzzers, Generic\n",
    ">[!NOTE]  \n",
    "> *Worms* can be considered DoS attacks as they aim to duplicate and overcharge the network.  \n",
    "> We classify *Generic* attacks under the Probe category, as they focus on information gathering and testing vulnerabilities (like *Brute Force Attack*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_cat'] = df['attack_cat'].str.strip()\n",
    "\n",
    "attack_categories = {\n",
    "    'DoS': ['DoS', 'Worms'],\n",
    "    'U2R': ['Backdoor', 'Shellcode'],\n",
    "    'R2L': ['Exploits', 'Analysis'],\n",
    "    'Probe': ['Reconnaissance', 'Fuzzers', 'Generic'],\n",
    "    'Normal': ['Normal']\n",
    "}\n",
    "\n",
    "def assign_attack_group(attack_cat):\n",
    "    for category, attacks in attack_categories.items():\n",
    "        if attack_cat in attacks:\n",
    "            return category\n",
    "    return 'Unknown' \n",
    "\n",
    "df['attack_cat'] = df['attack_cat'].apply(assign_attack_group)\n",
    "\n",
    "\n",
    "attack_group_count = df['attack_cat'].value_counts()\n",
    "attack_group_count.to_frame().reset_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#4CAF50', '#FF0000', '#FF0000', '#FF0000', '#FF0000']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "attack_group_count.plot(kind='barh' , color=colors)\n",
    "plt.title('Count of Instances per Attack Group')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('attack_cat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#4CAF50', '#FF0000', '#FF0000', '#FF0000', '#FF0000']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "attack_group_count.plot(kind='barh' , logx = True, color=colors)\n",
    "plt.title('Count of Instances per Attack Group (logarithmic scale)')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('attack_cat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81986c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['attack_cat'].isin(['DoS'])]\n",
    "filtered_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3108c39",
   "metadata": {},
   "source": [
    "As we can see now the dataset contains 4 categories for *attacks* and 1 for *benign* packets.  \n",
    "This could led to proceed in 2 different ways:\n",
    "- Balance the dataset using SMOTE. Be carefull to make both *oversampling* for *malicious* category and *undersampling* for *benign* one. This because the dataset is very **unbalanced** and the traformation made by using only *oversampling* will probably *broke* the dataset, because passing from *25k* to *281k* is a **very strong trasformation**.\n",
    "- Leave the dataset as it is and use *models* and *metrics* appropiated to an *unbalanced* dataset.\n",
    "\n",
    "> [!NOTE]\n",
    "> You shouldn't balanced the dataset as a whole, but only balance the training dataset and leave to test set as it is. Is important to don't make a prediction on a \"crafted\" test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06928442",
   "metadata": {},
   "source": [
    "## Nominal Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88720b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_categoric=[\n",
    "    'proto', 'state', 'service', 'attack_cat', 'Label'\n",
    "    ]\n",
    "\n",
    "df_filtered = df[df['Label'] == 1] \n",
    "df_filtered_count = (df_filtered[column_categoric].value_counts().to_frame().reset_index().rename(columns={0: 'count'}).head(15))\n",
    "df_filtered_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b4934",
   "metadata": {},
   "source": [
    "All the *udp* attack are in *INT state* and for *dns* service.  \n",
    "Most common *tcp* attack are *Exploits* and *Fuzzers* but in the *FIN* state. the *http* is the most attacked service. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b55be",
   "metadata": {},
   "source": [
    "### Analysis of IP and PORT attributes\n",
    "Check if there is some correlation between ip, port and attack to know if this two attributes are relevant for our analysis.<br>\n",
    "Starting from IP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8ff2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_df = df[df['Label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad905bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of unique source IPs in attacks: {attacks_df['srcip'].nunique()}\")\n",
    "srcip_counts = attacks_df['srcip'].value_counts()\n",
    "srcip_counts.head(45).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43762f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of unique destination IPs in attacks: {attacks_df['dstip'].nunique()}\")\n",
    "dstip_counts = attacks_df['dstip'].value_counts()\n",
    "dstip_counts.head(45).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab02367",
   "metadata": {},
   "source": [
    "Now look at port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ad112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of unique source ports in attacks: {attacks_df['sport'].nunique()}\")\n",
    "sport_counts = attacks_df['sport'].value_counts()\n",
    "sport_counts.head(45).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of unique source ports in attacks: {attacks_df['dsport'].nunique()}\")\n",
    "sport_counts = attacks_df['dsport'].value_counts()\n",
    "sport_counts.head(45).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391dc718",
   "metadata": {},
   "source": [
    "In the UNSW-NB15 dataset, the simulated attacks always originate from the same source IP addresses and are directed towards the same destination IP addresses, as described in the paper (https://ieeexplore.ieee.org/abstract/document/7348942). Therefore, the attributes srcip (Source IP address), dstip (Destination IP address), and sport (Source port number) have limited relevance for attack classification therefore in our case (lab data) will only create an **enormous** BIAS. These attributes do not reflect the variability of IP addresses and source ports observable in a real network environment, making them less informative.\n",
    "<br>\n",
    "On the other hand, the attribute dport (Destination port number) could be important but redundant, as there are other attributes like proto (protocol) and service that more or less carry the same information. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee023b2d",
   "metadata": {},
   "source": [
    "### Analysis of Stime and Ltime attributes \n",
    "The attributes Stime and Ltime, which represent the start timestamps and the end one of network events, are likely of low relevance for classifying attacks. These timestamps are specific to the moment of recording and do not follow deterministic patterns useful for distinguishing between normal traffic and attacks. Additionally, the connection duration (dur) makes these attributes redundant. We will probably not consider them in our initial model but will study them more thoroughly during the feature selection phase using the correlation matrix to confirm their irrelevance.  \n",
    "\n",
    "#### #TODO Analize Stime and Ltime in features selection using correlation matrix in feature selction phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978f07e",
   "metadata": {},
   "source": [
    "## Identify Missing and Erroneus Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values_filtered = missing_values[missing_values > 0]\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Missing values in each column:\")\n",
    "missing_values_filtered.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438f2e3",
   "metadata": {},
   "source": [
    "The unique colums with missing value are *ct_flw_http_mthd* and *is_ftp_login*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_missing_values = df[df.isnull().any(axis=1)]\n",
    "print(\"\\nNumber of rows with at least one missing values in one column:\", len(rows_with_missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = df.shape[0]\n",
    "print(f\"Total number of rows in the dataset: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47712f1",
   "metadata": {},
   "source": [
    "> [!Note]\n",
    "> We have at least one missing value for row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c7a15",
   "metadata": {},
   "source": [
    "### ct_flw_http_mthd and *service* attributes analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f9110",
   "metadata": {},
   "source": [
    "Probably the *ct_flw_http_mthd* (number of flows that has methods such as Get and Post in http service) are directly linked with *service* attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df[df['ct_flw_http_mthd'].isnull()]\n",
    "\n",
    "missing_values_by_service = missing_values['service'].value_counts()\n",
    "\n",
    "print(\"Number of missing values in ct_flw_http_mthd column for each service:\")\n",
    "missing_values_by_service.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea46cf5",
   "metadata": {},
   "source": [
    "if *ct_flw_http_mthd* is null no *http* in the column service is found. BUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_count = (df['service'] == 'http').sum()\n",
    "other_count = total_rows - http_count\n",
    "\n",
    "ct_flw_count = df['ct_flw_http_mthd'].notna().sum()\n",
    "\n",
    "print(f\"Number of rows without service 'http': {other_count}\")\n",
    "print(f\"Number of rows with service 'http': {http_count}\")\n",
    "print(f\"Number of rows with 'ct_flw_http_mthd': {ct_flw_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a65ac5",
   "metadata": {},
   "source": [
    "The number of raws without service 'http' is not really the number of raws missing con the attribute *ct_flw_http_mthd*.  \n",
    "Indeed the the number of raws not null of *ct_flw_http_mthd* is higher to the one with service *http* so some other service as *http* and this coul be an error in the dataset. The difference is **242**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flow_notna_values = df[df['ct_flw_http_mthd'].notna()]\n",
    "\n",
    "ct_flow_notna_values_no_http = ct_flow_notna_values[ct_flow_notna_values['service'] != 'http']\n",
    "\n",
    "service_counts = ct_flow_notna_values_no_http['service'].value_counts()\n",
    "\n",
    "service_counts.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190edbe",
   "metadata": {},
   "source": [
    "So there is an **error** and those service (-) should be **http**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e4e00",
   "metadata": {},
   "source": [
    "BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['service'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac58aa",
   "metadata": {},
   "source": [
    "AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2931259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ct_flow_notna_values_no_http.index, 'service'] = 'http'\n",
    "\n",
    "df['service'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d969",
   "metadata": {},
   "source": [
    "Now since the number of raws not null of the attribute *ct_flw_http_mthd* is equal to the packets with attribute *service* (33019) the attributes are correct.  \n",
    "Regarding the other services, they cannot have HTTP methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = ct_flow_notna_values['ct_flw_http_mthd'].min()\n",
    "max_value = ct_flow_notna_values['ct_flw_http_mthd'].max()\n",
    "\n",
    "print(f\"Numerical range for 'ct_flw_http_mthd': Min = {min_value}, Max = {max_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f206a11",
   "metadata": {},
   "source": [
    "Since the attribute *ct_flw_http_mthd* is a numerical value with those range, we can set the ct_flw_http_mthd value to 0 for rows where the service is one of these non-HTTP. This adjustment will be made during the preprocessing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_service_rows = df[df['service'] == '-']\n",
    "print(f\"Number of rows with unspecified service: {len(unspecified_service_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d800a2f",
   "metadata": {},
   "source": [
    "We can observe that there isn't missing value for the attribute *ct_flw_http_mthd* when the *service* is *http*.\n",
    "\n",
    "As we can see the attribute *service* as values '-' and this because those values are Missing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c7f5f",
   "metadata": {},
   "source": [
    "Now check if service erroneus value are linked with the attacks to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb283e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_service_rows = df[df['service'] == '-']\n",
    "total_unspecified = len(unspecified_service_rows)\n",
    "\n",
    "attack_rows = unspecified_service_rows[unspecified_service_rows['Label'] == 1]\n",
    "total_attacks = len(attack_rows)\n",
    "\n",
    "percentage_attack = (total_attacks / total_unspecified) * 100\n",
    "\n",
    "print(f'Total Attacks with Unspecified Service: {total_attacks}')\n",
    "print(f'Percentage of Attacks that have unspecified service:  {percentage_attack}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839bed1",
   "metadata": {},
   "source": [
    "Since several instances representing attacks have the value '-' in the service column, it would not be advisable to remove those rows. Therefore, as mentioned above, we will substitute the value in pre-processing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4aad09",
   "metadata": {},
   "source": [
    "### is_ftp_login attribute analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abc291",
   "metadata": {},
   "source": [
    "Probably also the is_ftp_login (if the ftp session is accessed by user and password then 1 else 0) are directly linked with service attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb89972",
   "metadata": {},
   "outputs": [],
   "source": [
    "unspecified_is_ftp_login_rows = df[df['is_ftp_login'].isnull()]\n",
    "print(f\"Number of rows with unspecified is_ftp_login: {len(unspecified_is_ftp_login_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92009752",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df[df['is_ftp_login'].isnull()]\n",
    "\n",
    "missing_values_by_service = missing_values['service'].value_counts()\n",
    "\n",
    "print(\"Number of missing values in is_ftp_login column for each service:\")\n",
    "missing_values_by_service.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477690e7",
   "metadata": {},
   "source": [
    "The value should be binary, but we have different integer values. Look at the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51918c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ftp_login_df = df['is_ftp_login'].value_counts()\n",
    "is_ftp_login_df.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94253f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_missing_is_ftp_login_rows = df[df['is_ftp_login'].notnull()]\n",
    "non_missing_is_ftp_login_rows_ftp = non_missing_is_ftp_login_rows[non_missing_is_ftp_login_rows['service'] == 'ftp']\n",
    "service_count = non_missing_is_ftp_login_rows_ftp['service'].value_counts()\n",
    "service_count.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591d8f6",
   "metadata": {},
   "source": [
    "We can conclude that all non-missing values of is_ftp_login are within the ftp service. Therefore, during the preprocessing phase, we can substitute all positive values for this attribute with 1 and set the missing values to 0.  \n",
    "\n",
    "The *is_ftp_login* attributes probably was only set to a value if it was present and was missing in all the other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_missing_is_ftp_login_null = df[df['is_ftp_login'].isnull()]\n",
    "\n",
    "non_missing_is_ftp_login_rows_attack = non_missing_is_ftp_login_null[(non_missing_is_ftp_login_null['Label'] == 1) & (non_missing_is_ftp_login_null['service'] == 'ftp')]\n",
    "service_count = non_missing_is_ftp_login_rows_attack['Label'].value_counts()\n",
    "\n",
    "print(\"Number of attacks with missing is_ftp_login and service as ftp:\")\n",
    "service_count.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7cd84",
   "metadata": {},
   "source": [
    "Our first thought was to eliminate the column as it can say the same things for classifying an attack to *ftp*. But this is not true, indeed there are some missing values for *is_ftp_login* in the *ftp* service that belongs to the class label *1* (attack). So eliminating the column will only result in a loss of information.  \n",
    "There are in *ftp* some attacks that does not depend on the fact that the user is accessed or not (*is_ftp_login*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42bd1cc",
   "metadata": {},
   "source": [
    "### ct_ftp_cmd attribute analysis\n",
    "Numbers of flows that has a command in ftp session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_ftp_cmd_df = df['ct_ftp_cmd'].value_counts()\n",
    "ct_ftp_cmd_df.to_frame().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2329e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_ct_ftp_cmd_rows = df[df['ct_ftp_cmd'] != ' ']\n",
    "non_empty_ct_ftp_cmd_rows_ftp = non_empty_ct_ftp_cmd_rows[non_empty_ct_ftp_cmd_rows['service'] == 'ftp']\n",
    "service_count = non_empty_ct_ftp_cmd_rows_ftp['service'].value_counts()\n",
    "service_count.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79765082",
   "metadata": {},
   "source": [
    "We observe that all rows with a 'ct_ftp_cmd' value different from null have 'ftp' as the 'service' attribute. Therefore, we can substitute the null values with 0. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ct_ftp_cmd'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30af6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_ftp_login'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df\n",
    "test['ct_ftp_cmd'] = pd.to_numeric(df['ct_ftp_cmd'], errors='coerce').astype('Int64')\n",
    "\n",
    "non_zero_rows = test[(test['is_ftp_login'].notna()) & (test['ct_ftp_cmd'] != ' ')]\n",
    "\n",
    "are_values_equal = (non_zero_rows['is_ftp_login'] == non_zero_rows['ct_ftp_cmd']).all()\n",
    "\n",
    "are_values_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f00571",
   "metadata": {},
   "source": [
    "Indeed analysing those two attributes *is_ftp_login*, *ct_ftp_cmd* we notice that they are equals in number, in values and in raws.  \n",
    "Probably one of them is wrong, even if not anyway the attributes together are redundant. So we decided to remove *is_ftp_login* that is probably the erroneous value beacause the documentation says that it should be a *Bynary* tipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af7628",
   "metadata": {},
   "source": [
    "#### #TODO Remove the columns *is_ftp_login* because is redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b1d7e",
   "metadata": {},
   "source": [
    "Regarding all the consideration made for the attribute *is_ftp_login* they remain still valid because we can just apply that to the attribute *ct_ftp_cmd* indeed they are actually the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cf408",
   "metadata": {},
   "source": [
    "## Overall overview of the values in the dataset\n",
    "\n",
    "### Nominal Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "print(\"Column with string values:\")\n",
    "string_columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = df[string_columns].nunique()\n",
    "print(\"\\nNumber of unique values in each nominal column:\")\n",
    "unique_values_count.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c70ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique values in each nominal column:\")\n",
    "for column in string_columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91919f",
   "metadata": {},
   "source": [
    "### Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "print(\"Columns with numeric values:\")\n",
    "print(numeric_columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_count = df[numeric_columns].nunique()\n",
    "print(\"\\nNumber of unique values in each numerical column:\")\n",
    "unique_values_count.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique values in each numerical column:\")\n",
    "for column in numeric_columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e231ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USE ONLY 10% of df\n",
    "###df = df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e8ebf",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Already done:\n",
    "- Delete duplicates. \n",
    "- Add value in *attack_cat* for Normal packets.\n",
    "- Substitued the attribute *attack_cat* with new macro-categories.\n",
    "- Resolved incongruence between attributes *ct_flw_http_mthd* and *service*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e9cfd",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3ed0d",
   "metadata": {},
   "source": [
    "### Remove IP and Ports.\n",
    "\n",
    "Delete attributes for source and destination *ip addresses* and *ports* (*srcip*, *dstip*, *sport*, *dsport*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['srcip', 'dstip', 'sport', 'dsport'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adff79d",
   "metadata": {},
   "source": [
    "### Missing values in attribute *service*\n",
    "\n",
    "Substitute '-' with 'missing' in the attribute *service*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_missing_value = df[df['service'] == '-']\n",
    "\n",
    "df.loc[service_missing_value.index, 'service'] = 'missing'\n",
    "\n",
    "df['service'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5164a3",
   "metadata": {},
   "source": [
    "### Missing values in attribute *ct_flw_http_mthd*\n",
    "\n",
    "Substitute missing values with *0.0* in the attribute *ct_flw_http_mthd*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_flow_missing_value = df[df['ct_flw_http_mthd'].isnull()]\n",
    "\n",
    "df.loc[ct_flow_missing_value.index, 'ct_flw_http_mthd'] = 0.0\n",
    "\n",
    "df['ct_flw_http_mthd'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dc8b7",
   "metadata": {},
   "source": [
    "### Missing values in attribute *ct_ftp_cmd* \n",
    "As the documentations says this attribute should be *Integer* but it's a *string*. Firstly we will convert values in string and then fill the *Missing Values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef15c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ct_ftp_cmd'] = pd.to_numeric(df['ct_ftp_cmd'], errors='coerce').astype('Int64')\n",
    "\n",
    "ct_ftp_missing_value = df[df['ct_ftp_cmd'].isnull()]\n",
    "\n",
    "df.loc[ct_ftp_missing_value.index, 'ct_ftp_cmd'] = 0.0\n",
    "\n",
    "df['ct_ftp_cmd'].value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a789f76",
   "metadata": {},
   "source": [
    "### Remove *is_ftp_login* attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17eafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['is_ftp_login'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6fd2a7",
   "metadata": {},
   "source": [
    "Drop duplicate again because we removed some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c25b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90284a",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5952e9",
   "metadata": {},
   "source": [
    "Create the trainig test dataframe. We don't need Label because we are classifying with *attack_cat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79c7ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = df['attack_cat']\n",
    "train_x_raw = df.drop(['attack_cat','Label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a8e5b",
   "metadata": {},
   "source": [
    "Update colums lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = train_x_raw.select_dtypes(include=['object']).columns\n",
    "numeric_columns = train_x_raw.select_dtypes(include=['number']).columns\n",
    "\n",
    "string_columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79667e8a",
   "metadata": {},
   "source": [
    "Use *pd.get_dummies* method because **SMOTE** and **Logistc Regression** Classifier cannot handle nominal columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.get_dummies(train_x_raw, columns=string_columns, drop_first=True)\n",
    "dummy_variables = list(set(train_x)-set(train_x_raw))\n",
    "\n",
    "print(dummy_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cac887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_frequency = train_x_raw.copy()\n",
    "for col in string_columns:\n",
    "    # Compute the frequency of each category in the column\n",
    "    frequency_encoding = train_x_frequency[col].value_counts(normalize=True)\n",
    "    \n",
    "    # Replace each category in the column with its frequency\n",
    "    train_x_frequency[f'{col}_Encoded'] = train_x_frequency[col].map(frequency_encoding)\n",
    "\n",
    "# Show the result\n",
    "train_x_frequency = train_x_frequency.drop(columns=string_columns)\n",
    "train_x_frequency.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24669e82",
   "metadata": {},
   "source": [
    "### Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01d03661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a611c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.astype('float64')\n",
    "\n",
    "train_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d026c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels with NO sampler:')\n",
    "train_Y.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy = {\"Normal\":100000} #set to 10000 if use 10% of df\n",
    "under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "train_x_under, train_Y_under = under_sampler.fit_resample(train_x, train_Y)\n",
    "\n",
    "train_Y_under.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "train_x_balanced_100k, train_Y_balanced_100k = smote.fit_resample(train_x_under, train_Y_under)\n",
    "\n",
    "train_Y_balanced_100k.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd47ab1",
   "metadata": {},
   "source": [
    "### Feature Selection (general)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084c308",
   "metadata": {},
   "source": [
    "Using Decision tree to sort features for importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e12239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# model.fit(train_x_balanced_100k, train_Y_balanced_100k)\n",
    "\n",
    "# importances = pd.Series(model.feature_importances_, index=train_x_balanced_100k.columns)\n",
    "\n",
    "# selected_features = importances.sort_values(ascending=False).reset_index()\n",
    "# selected_features.columns = ['Feature', 'Importance']\n",
    "\n",
    "\n",
    "# selected_features_worst_10_dec = selected_features.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56a4dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(train_x_balanced_100k)\n",
    "\n",
    "# log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# log_reg_model.fit(X_train_scaled, train_Y_balanced_100k)\n",
    "\n",
    "# importances = pd.Series(np.abs(log_reg_model.coef_[0]), index=train_x_balanced_100k.columns)\n",
    "\n",
    "# selected_features = importances.sort_values(ascending=False).reset_index()\n",
    "# selected_features.columns = ['Feature', 'Importance']\n",
    "\n",
    "# selected_features_worst_10_reg = selected_features.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce3496d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_worst_features = selected_features_worst_10_reg.merge(selected_features_worst_10_dec, on=\"Feature\")\n",
    "\n",
    "# print(common_worst_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9284475",
   "metadata": {},
   "source": [
    "Use Corrrelation matrix to remove the most useless features to characterized a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30261280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b649b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr = df.drop(['Label'], axis=1)\n",
    "# df_corr = pd.get_dummies(df_corr, columns=string_columns, drop_first=True)\n",
    "\n",
    "# df_corr.loc[df_corr['attack_cat'] == 'Normal', 'attack_cat'] = 0\n",
    "# df_corr.loc[df_corr['attack_cat'] == 'DoS', 'attack_cat'] = 1\n",
    "# df_corr.loc[df_corr['attack_cat'] == 'Probe', 'attack_cat'] = 2\n",
    "# df_corr.loc[df_corr['attack_cat'] == 'R2L', 'attack_cat'] = 3\n",
    "# df_corr.loc[df_corr['attack_cat'] == 'U2R', 'attack_cat'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdf48250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix = df_corr.corr()\n",
    "# correlation_attack_cat = correlation_matrix['attack_cat'].sort_values(ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(correlation_attack_cat.to_frame(), cmap=\"coolwarm\", linewidths=0.5)\n",
    "# plt.title(\"Correlation with 'attack_cat'\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fe5a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_with_target = correlation_matrix['attack_cat']\n",
    "\n",
    "# # Set a threshold for low correlation\n",
    "# threshold = 0.005\n",
    "\n",
    "# features_to_keep = correlation_with_target[abs(correlation_with_target) > threshold].index\n",
    "\n",
    "# features_eliminated = correlation_with_target[abs(correlation_with_target) <= threshold].index\n",
    "\n",
    "# print(features_eliminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db98bc4-6b9c-46ee-be6f-65243e241e9c",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f04df9-ef26-48a5-80f2-f6c8e0ff6da6",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d83091b-387f-40ca-b838-d295f1febab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f9cb5",
   "metadata": {},
   "source": [
    "### Unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5939a0-fcf2-4e3f-a867-79bdd0d27ff3",
   "metadata": {},
   "source": [
    "Split the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08c2e6b2-a097-4776-8474-23aa28ba68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x_frequency, train_Y, test_size=0.30, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fe165-e145-42e5-b8ec-08e15d1d297c",
   "metadata": {},
   "source": [
    "Scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4636eba1-2590-41d6-8adc-6ad59282a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318410cd-a49c-44db-b48e-ebc01cbfd970",
   "metadata": {},
   "source": [
    "Create and train the logistic regression model and make prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f93cfdf2-4e45-4116-b1dd-6c3caa471ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30d020-cb3c-4322-bfd0-71405f5666fd",
   "metadata": {},
   "source": [
    "Evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8681f-3960-460c-9c90-ed7700b3c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------\")\n",
    "print(f\"Accuracy = {accuracy_score(y_pred, y_test)}\")\n",
    "print(\"--------------------------\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2f01e-60ca-4a78-83db-2f290d599618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(clf, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69b904-1359-45f1-b20e-5901fc52df40",
   "metadata": {},
   "source": [
    "Extracting Coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33127c37-6798-4cee-9b22-ab82e3d81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = clf.coef_[0]\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "coeff_df['Abs_Coefficient'] = coeff_df['Coefficient'].abs()\n",
    "coeff_df = coeff_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "coeff_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5ef9e-fb93-4d8e-9293-241c946fc99e",
   "metadata": {},
   "source": [
    "Accuracy: The model achieves an accuracy of 96.3%, but this metric is misleading due to class imbalance. The \"Normal\" class dominates, leading to high performance for this class but poor detection of attack classes.\n",
    "\n",
    "Class Imbalance: The model performs exceptionally well on the \"Normal\" class but struggles with attack classes, especially DoS and U2R, showing very low recall and F1-scores for these categories (near zero for U2R).\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "- Class Balancing: Addressing class imbalance through oversampling or undersampling of minority classes will improve performance on attack detection.\n",
    "- Feature Selection: Not use all features but only top features based on large absolute coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd8f2-c84e-4cf8-8e6b-fbe8eabe6ce0",
   "metadata": {},
   "source": [
    "Now use the balanced dataframe with cross Validation. Balance only the training set made by CV avery time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7996db-26c5-4b4b-8ab9-73edf8984930",
   "metadata": {},
   "source": [
    "### Balanced Dataset and CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbcd85e5-89ff-4ace-9650-075b20c0928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99965784-ef04-4c1a-952f-4d2e8124ffff",
   "metadata": {},
   "source": [
    "Use cross validation with 10 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2162088-97ed-4f6e-b556-0d0919d46bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83c0d1",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf5814a5-cf8e-4ff5-b098-af98249ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3487e-a843-4afa-b6fa-1a653b9a8aaa",
   "metadata": {},
   "source": [
    "Use cross validation with undersampling and SMOTE for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfa750-f055-48a8-b3e0-e9341c5fed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x, train_Y)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x.iloc[train_idx], train_x.iloc[test_idx]\n",
    "    y_train, y_test = train_Y.iloc[train_idx], train_Y.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {\"Normal\": 100000} #set to 10000 if use 10% of df\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28b9c1-49d2-41c3-9f5e-4eb87aa94791",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f083b12-8e2f-4f33-94a1-f73408946257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df.melt(id_vars=['Fold'], \n",
    "                                 value_vars=['Accuracy', 'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)']), \n",
    "            x='variable', y='value', hue='Fold', palette='viridis')\n",
    "plt.title(\"Performance Metrics Across Folds\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.legend(title=\"Fold\", loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bd97d",
   "metadata": {},
   "source": [
    "### Logistic Regression using Frequency encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfa841",
   "metadata": {},
   "source": [
    "Logistic Regression withouth dummies but using Frequency encoding to substitute nominal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07deb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "train_x_frequency = train_x_frequency.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fe03c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x_frequency, train_Y)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x_frequency.iloc[train_idx], train_x_frequency.iloc[test_idx]\n",
    "    y_train, y_test = train_Y.iloc[train_idx], train_Y.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {\"Normal\": 100000}\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d78e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e14e6",
   "metadata": {},
   "source": [
    "### Feature Selection (logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5778f",
   "metadata": {},
   "source": [
    "In order to tacking into account that Logistic Regression don't work well if the attributes of the dataset are not indipendent, here we do a specific feature selection using correlation matrix not apply to the label but to the attributes of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_x.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59481501",
   "metadata": {},
   "source": [
    "Identify features that are very correlated (0.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a3c2d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_highly_correlated_features(data, threshold=0.9):\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper_triangle_corr = corr_matrix.where(upper_triangle)\n",
    "\n",
    "    highly_correlated_pairs = []\n",
    "    for column in upper_triangle_corr.columns:\n",
    "        for index in upper_triangle_corr.index:\n",
    "            if upper_triangle_corr.at[index, column] is not np.nan and abs(upper_triangle_corr.at[index, column]) > threshold:\n",
    "                highly_correlated_pairs.append((column, index, upper_triangle_corr.at[index, column]))\n",
    "\n",
    "    return highly_correlated_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of total features: {train_x.shape[1]}\")\n",
    "threshold = 0.8\n",
    "highly_correlated_pairs = identify_highly_correlated_features(train_x, threshold=threshold)\n",
    "\n",
    "print(f\"Highly correlated feature pairs (correlation > {threshold}):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(f\"'{pair[0]}' and '{pair[1]}' --> correlated more than {threshold} with a correlation of {pair[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912585af",
   "metadata": {},
   "source": [
    "We choose to remove: sloss, dloss, Dpkts, dwin, Ltime, tcprtt, is_sm_ips_ports, ct_dst_ltm, ct_srv_dst, ct_dst_src_ltm, ct_ftp_cmd, ct_flw_http_mthd, swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_deleted_logistic = train_x.drop(['sloss', 'dloss', 'Dpkts', 'dwin', 'Ltime', 'tcprtt', 'is_sm_ips_ports', 'ct_dst_ltm', 'ct_srv_dst', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'swin'], axis=1)\n",
    "print(f\"Number of total features: {train_x_deleted_logistic.shape[1]}\")\n",
    "train_x_deleted_logistic_frequency = train_x_frequency.drop(['sloss', 'dloss', 'Dpkts', 'dwin', 'Ltime', 'tcprtt', 'is_sm_ips_ports', 'ct_dst_ltm', 'ct_srv_dst', 'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'swin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b560c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x_deleted_logistic, train_Y)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x_deleted_logistic.iloc[train_idx], train_x_deleted_logistic.iloc[test_idx]\n",
    "    y_train, y_test = train_Y.iloc[train_idx], train_Y.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {\"Normal\": 100000}\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80ca39",
   "metadata": {},
   "source": [
    "### Logistic Regression with frequency encoding (feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3221c3",
   "metadata": {},
   "source": [
    "Deleted high correlated attributes and using frequency encoding not dummies.  \n",
    "At the end of the story the model is faster but still mislead in classify the *DoS* and *U2R* attack category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b403729",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x_deleted_logistic_frequency, train_Y)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x_deleted_logistic_frequency.iloc[train_idx], train_x_deleted_logistic_frequency.iloc[test_idx]\n",
    "    y_train, y_test = train_Y.iloc[train_idx], train_Y.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {\"Normal\": 100000}\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7527cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95713661",
   "metadata": {},
   "source": [
    "## Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269bc75",
   "metadata": {},
   "source": [
    "Since we noticed that using Multinomial Logistic Regression, the model struggles to classify DOS and U2R attacks effectively, we have decided to try binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "250a29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_binary = train_Y.apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98864fd7",
   "metadata": {},
   "source": [
    "### Balanced Dataset and CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0089638",
   "metadata": {},
   "source": [
    "Use cross validation with 10 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be57b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3603fad",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a041adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f8ede",
   "metadata": {},
   "source": [
    "Use cross validation with undersampling and SMOTE for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x, train_Y_binary)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x.iloc[train_idx], train_x.iloc[test_idx]\n",
    "    y_train, y_test = train_Y_binary.iloc[train_idx], train_Y_binary.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {0: 100000} #set to 10000 if use 10% of df\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    clf.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df.melt(id_vars=['Fold'], \n",
    "                                 value_vars=['Accuracy', 'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']), \n",
    "            x='variable', y='value', hue='Fold', palette='viridis')\n",
    "plt.title(\"Performance Metrics Across Folds\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.legend(title=\"Fold\", loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b7f68",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Number of Decision tree = 100 and using Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "020e155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f7a17aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "669ab3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c144f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(train_x, train_Y)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    X_train_raw, X_test_raw = train_x.iloc[train_idx], train_x.iloc[test_idx]\n",
    "    y_train, y_test = train_Y.iloc[train_idx], train_Y.iloc[test_idx]\n",
    "    \n",
    "    # Undersample only the training set\n",
    "    sampling_strategy = {\"Normal\": 100000} #set to 10000 if use 10% of df\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_train_under, y_train_under = under_sampler.fit_resample(X_train_raw, y_train)\n",
    "    \n",
    "    # SMOTE the other parts\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_under, y_train_under)\n",
    "    \n",
    "    rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test_raw)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fold_results.append({\n",
    "        'Fold': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precision (weighted)': report['weighted avg']['precision'],\n",
    "        'Recall (weighted)': report['weighted avg']['recall'],\n",
    "        'F1-Score (weighted)': report['weighted avg']['f1-score'],\n",
    "        'Precision (macro)': report['macro avg']['precision'],\n",
    "        'Recall (macro)': report['macro avg']['recall'],\n",
    "        'F1-Score (macro)': report['macro avg']['f1-score'],\n",
    "        'Confusion Matrix': cm\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for Fold {fold + 1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e930b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "summary_stats = results_df[\n",
    "    ['Accuracy', \n",
    "     'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)', \n",
    "     'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)']\n",
    "].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\n--- Overall Performance ---\")\n",
    "print(summary_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aivirtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
